{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import third module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import string\n",
    "import sys\n",
    "import fileinput\n",
    "import json\n",
    "import urllib\n",
    "import urllib3\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "from termcolor import colored, cprint\n",
    "import colorama\n",
    "import webbrowser\n",
    "import base64\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_predict, learning_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from first_part import URLFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **get_learning_curve** is used for plotting the train_accuracy,valiadation_accuracy when number of training samples change\n",
    "  - **plot_confusion_matrix** is used for plotting the confusion matrix\n",
    "  - **plot_ROC_CURVE** is used for plotting the ROC CURVE \n",
    "  - **plot_pca** is used for plotting the explanined variance of componets\n",
    "  - all above function will be  called in **train_model** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_curve(X, y):\n",
    "    clf=ske.RandomForestClassifier(n_estimators=50)\n",
    "    parameter_grid = np.array([200, 500, 800, 1100])\n",
    "    train_size, train_scores, validation_scores = learning_curve(clf, X, y, train_sizes=parameter_grid, cv=5)\n",
    "\n",
    "\n",
    "    # plot the data\n",
    "    plt.figure(figsize=(10, 8), dpi=80)\n",
    "    plt.plot(parameter_grid, 100 * np.average(train_scores, axis=1), color='yellow',label='train accuracy')\n",
    "    plt.plot(parameter_grid, 100 * np.average(validation_scores,axis=1),color='red',label='validation accuracy')\n",
    "    plt.plot(parameter_grid,100*np.repeat((np.average(validation_scores,axis=1)[-1]+np.average(train_scores, axis=1)[-1])/2,6),color='blue',linestyle='--',label='desired accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Learning curve')\n",
    "    plt.xlabel('Number of training samples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([50, 100])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels_name, title):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]    # Normalize\n",
    "    plt.imshow(cm, interpolation='nearest')    # Display images on specific windows\n",
    "    plt.title(title)    # Title of the image\n",
    "    plt.colorbar()\n",
    "    num_local = np.array(range(len(labels_name)))\n",
    "    plt.xticks(num_local, labels_name, rotation=90)    \n",
    "    plt.yticks(num_local, labels_name)    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_CURVE(y,Y):\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, threshold = roc_curve(y, Y)  ###Calculate TPR and FPR\n",
    "    roc_auc = auc(fpr, tpr)  ###calculate ROC\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)  ###FPR as xï¼ŒTPR as y\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot the explaned variance of componet\n",
    "def plot_pca(X):   \n",
    "    cov_mat = np.cov(X.T)\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "    tot = sum(eigen_vals)\n",
    "    var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    plt.bar(range(1, 1+len(var_exp)), var_exp, alpha=0.5, align='center',label='individual explained variance')\n",
    "    plt.step(range(1, 1+len(var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.title('Explained variance of componets')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part: define the train_model function\n",
    "- In this function,we will usd randomforest to train the model with data from part-I and visiualize the result with funciton defined above\n",
    "- Using joblib,we store our trained model in pkl file,so that we can call it from other py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Creates a pandas dataframe and reads in the URL dataset with\n",
    "    # extracted features. \n",
    "    df = pd.read_csv('data_urls.csv', sep='|')\n",
    "\n",
    "    # Assigns X to features. Drops URL name and label.\n",
    "    X = df.drop(['URL', 'Malicious','Safebrowsing'], axis=1).values\n",
    "    \n",
    "    # Assigns y to labels.\n",
    "    y = df['Malicious'].values\n",
    "\n",
    "    # Split data into training and test datasets.\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "    # Print the number of training and testing samples.\n",
    "    print(\"\\t Training samples: \", len(X_train))\n",
    "    print(\"\\t Testing samples: \", len(X_test))\n",
    "    \n",
    "    ##Standardize\n",
    "    s = StandardScaler()\n",
    "    X_train_scale = s.fit_transform(X_train)\n",
    "    X_test_scale = s.transform(X_test)\n",
    "\n",
    "    # Train Random forest algorithm on training dataset.\n",
    "    clf = ske.RandomForestClassifier(n_estimators=50)   \n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    # Perform cross validation and print out accuracy.\n",
    "    score = model_selection.cross_val_score(clf, X_test, y_test, cv=10)\n",
    "    print(\"\\n\\t Cross Validation Score: \", round(score.mean()*100, 2), '%')\n",
    "    \n",
    "    \n",
    "    # Calculate f1 score.\n",
    "    y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "    f = f1_score(y_train, y_train_pred)\n",
    "    print(\"\\t F1 Score: \", round(f*100, 2), '%')\n",
    "   \n",
    "    plot_ROC_CURVE(y_train, y_train_pred)\n",
    "\n",
    "    # plot the matrix\n",
    "\n",
    "    cm=confusion_matrix(y_train,y_train_pred)\n",
    "    plot_confusion_matrix(cm,[\"malicious\",\"benign\"],\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    ##Plot the explaned variance of componet\n",
    "    plot_pca(X_train_scale)\n",
    "\n",
    "    # plot learning curve\n",
    "    get_learning_curve(X_train,y_train)\n",
    "    # Save the configuration of the classifier and features as a pickle file.\n",
    "    \n",
    "    all_features = X.shape[1]\n",
    "    features = []\n",
    "\n",
    "    for feature in range(all_features):\n",
    "        features.append(df.columns[1+feature])\n",
    "\n",
    "    try:\n",
    "        print(\"\\n Saving algorithm and feature list in classifier directory...\")\n",
    "        joblib.dump(clf, 'classifier.pkl')\n",
    "        open('features.pkl', 'wb').write(pickle.dumps(features))\n",
    "        print(colored('\\n[*] ', 'green') + \" Saved.\")\n",
    "    except:\n",
    "        print('\\n Error: Algorithm and feature list not saved correctly.\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call the function and show the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa8 in position 35: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa8 in position 35: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-e16f6dbe5f6a>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Creates a pandas dataframe and reads in the URL dataset with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# extracted features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_urls.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Assigns X to features. Drops URL name and label.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa8 in position 35: invalid start byte"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model above to identify  whether the url  is malicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **get_url_info** function extracts features from a user supplied.The function extracts all features similarly to extract_features(),but instead saves the extracted features in the form of a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL information function extracts features from a user supplied\n",
    "# URL. The function extracts all features similarly to extract_features()\n",
    "# but instead saves the extracted features in the form of a dictionary. \n",
    "def get_url_info(url):\n",
    "    # Creates a dictionary for features to be stored in.\n",
    "    features = {}\n",
    "    \n",
    "    # Parses input URL to remove http:// or https://.\n",
    "    # The umbrella dataset does not contain this and thus,\n",
    "    # is not required for certain feature extractions.\n",
    "    parsed_url = parse_url(url)\n",
    "\n",
    "    # Retrieve URL entropy and store in dictionary.\n",
    "    getentropy = URLFeatures(parsed_url)\n",
    "    entropy = getentropy.Entropy()\n",
    "    features['Entropy'] = entropy\n",
    "\n",
    "    feature = URLFeatures()\n",
    "\n",
    "    # Store Bag Of Words in dictionary.  \n",
    "    features['BagOfWords'] = feature.bag_of_words(parsed_url)\n",
    "    \n",
    "    # Store Contains IP address in dictionary.\n",
    "    features['ContainsIP'] = feature.contains_IP(parsed_url)\n",
    "\n",
    "    # Store URL length in dictionary.\n",
    "    features['LengthURL'] = feature.url_length(parsed_url)\n",
    "\n",
    "    # Store amount of special characters in dictionary.\n",
    "    features['SpecialChars'] = feature.special_chars(parsed_url)\n",
    "\n",
    "    # Store amount of suspicious strings in dictionary.\n",
    "    features['SuspiciousStrings'] = feature.suspicious_strings(url)\n",
    "\n",
    "    # Store number of digits within the URL in dictionary.\n",
    "    features['NumberOfDigits'] = feature.num_digits(parsed_url)\n",
    "\n",
    "    # Store site popularity in dictionary.\n",
    "    features['Popularity'] = feature.popularity(parsed_url)\n",
    "\n",
    "    # Store Google Safebrowsing verdict in dictionary.\n",
    "    #apikey = base64.b64decode('QUl6YVN5QV9XbU53MHRyZTEybWtMOE1qYUExY0c3Smd4SnRuU0lv')\n",
    "    #apikey = apikey.decode('utf-8')\n",
    "    #safe = SafeBrowse(apikey)\n",
    "    #features['Safebrowsing'] = safe.threat_matches_find(url) \n",
    "\n",
    "    # Return features dictionary.\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **classify_url** function passes in the input URL and classifies it as malicious or benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_url(url):\n",
    "\n",
    "    # Loads classifier and feature configurations.\n",
    "    clf = joblib.load(os.path.join(\n",
    "    os.path.dirname(os.path.realpath(__file__)),\n",
    "    'classifier.pkl'))\n",
    "\n",
    "    features = pickle.loads(open(os.path.join(\n",
    "        os.path.dirname(os.path.realpath(__file__)),\n",
    "        'features.pkl'),\n",
    "        'rb').read())\n",
    "    \n",
    "    # Extracts features from input URL.\n",
    "    data = get_url_info(url)\n",
    "    feature_list = list(map(lambda x:data[x], features))\n",
    "\n",
    "    # Classifies input URL as malicious or benign.\n",
    "    result = clf.predict([feature_list])[0]\n",
    "\n",
    "    if result == 0:\n",
    "        print(\"MLURL has classified URL %s as \" % url + colored(\"benign\", 'green') + '.')\n",
    "    else: \n",
    "        print(\"MLURL has classified URL %s as \" % url + colored(\"malicious\", 'red') + '.')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **check_valid_url** function checks whether or not the input URL to classify is in a valid format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid URL function checks whether or not the input\n",
    "# URL to classify is in a valid format.\n",
    "\n",
    "def check_valid_url(url):\n",
    "    print(\"\\n[+] Validating URL format...\")\n",
    "    reg = re.compile('^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$')\n",
    "    \n",
    "    if re.match(reg, str(url)):\n",
    "        print(\"URL is valid.\")\n",
    "    else:\n",
    "        print(\"Error: URL is not valid. Please input a valid URL format. \")\n",
    "        sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
